-name=input("what's your age")\nname
-1+2
-print("hello")
-hello
-name+input("what's your name:")
-x=
-name=input("what's your name:")
-fff
-name=input("what's your name:")
-1
-hello
-1-2
-9*78+4*50
-help and
-help(and)
-help
-help> and
-help
-help 1 and 2
-help(1 and 2)
-1 and 2
-if 1 and 2
-if 1 and 2;
-if 1 and 2:
-print("True")
-if 1 and 2:
-print"true"
-if 1 and 2:
-	print("True")
-a=1
-if a=1:
-a=1
-if a==1:
-	print("a=1")
-else print("a=2")
-a=1\n>>> if a==1:\n... 	print("a=1")\n... else: print("a=2")
-if a==1
-if a==1:
-print("a=1")
-if a==1:
-	print("a=1")
-else:
-	print("a<>1")
-a=1
-if a==1:\n... 	print("a=1")\n... else:\n... 	print("a<>1")
-math.pi
-from sgmllib import SGMLParser\nimport sys,urllib2,urllib,cookielib\nclass spider(SGMLParser):\n    def __init__(self,email,password):\n        SGMLParser.__init__(self)\n        self.h3=False\n        self.h3_is_ready=False\n        self.div=False\n        self.h3_and_div=False\n        self.a=False\n        self.depth=0\n        self.names=""\n        self.dic={}   \n         \n        self.email=email\n        self.password=password\n        self.domain='renren.com'\n        try:\n            cookie=cookielib.CookieJar()\n            cookieProc=urllib2.HTTPCookieProcessor(cookie)\n        except:\n            raise\n        else:\n            opener=urllib2.build_opener(cookieProc)\n            urllib2.install_opener(opener)       \n\n    def login(self):\n        url='http://www.renren.com/PLogin.do'\n        postdata={\n                  'email':self.email,\n                  'password':self.password,\n                  'domain':self.domain  \n                  }\n        req=urllib2.Request(\n                            url,\n                            urllib.urlencode(postdata)            \n                            )\n        \n        self.file=urllib2.urlopen(req).read()\n        #print self.file\n    def start_h3(self,attrs):\n        self.h3 = True\n    def end_h3(self):\n        self.h3=False\n        self.h3_is_ready=True\n        \n    def start_a(self,attrs):\n        if self.h3 or self.div:\n            self.a=True\n    def end_a(self):\n        self.a=False\n        \n    def start_div(self,attrs):\n        if self.h3_is_ready == False:\n            return\n        if self.div==True:\n            self.depth += 1\n            \n        for k,v in attrs:\n            if k == 'class' and v == 'content':\n                self.div=True;\n                self.h3_and_div=True   #h3 and div is connected\n    def end_div(self):\n        if self.depth == 0:\n            self.div=False\n            self.h3_and_div=False\n            self.h3_is_ready=False\n            self.names=""\n        if self.div == True:\n            self.depth-=1\n    def handle_data(self,text):\n        #record the name\n        if self.h3 and self.a:\n            self.names+=text\n        #record says\n        if self.h3 and (self.a==False):\n            if not text:pass\n            else: self.dic.setdefault(self.names,[]).append(text)\n            return \n        if self.h3_and_div:\n            self.dic.setdefault(self.names,[]).append(text)\n            \n    def show(self):\n        type = sys.getfilesystemencoding()\n        for key in self.dic:\n            print ( (''.join(key)).replace(' ','')).decode('utf-8').encode(type), \\\n                  ( (''.join(self.dic[key])).replace(' ','')).decode('utf-8').encode(type)\n\n\n\n\nrenrenspider=spider('your email','your password')\nrenrenspider.login()\nrenrenspider.feed(renrenspider.file)\nrenrenspider.show()
-from sgmllib import SGMLParser\nimport sys,urllib2,urllib,cookielib\nclass spider(SGMLParser):\n    def __init__(self,email,password):\n        SGMLParser.__init__(self)\n        self.h3=False\n        self.h3_is_ready=False\n        self.div=False\n        self.h3_and_div=False\n        self.a=False\n        self.depth=0\n        self.names=""\n        self.dic={}   \n         \n        self.email=email\n        self.password=password\n        self.domain='renren.com'\n        try:\n            cookie=cookielib.CookieJar()\n            cookieProc=urllib2.HTTPCookieProcessor(cookie)\n        except:\n            raise\n        else:\n            opener=urllib2.build_opener(cookieProc)\n            urllib2.install_opener(opener)       \n\n    def login(self):\n        url='http://www.renren.com/PLogin.do'\n        postdata={\n                  'email':self.email,\n                  'password':self.password,\n                  'domain':self.domain  \n                  }\n        req=urllib2.Request(\n                            url,\n                            urllib.urlencode(postdata)            \n                            )\n        \n        self.file=urllib2.urlopen(req).read()\n        #print self.file\n    def start_h3(self,attrs):\n        self.h3 = True\n    def end_h3(self):\n        self.h3=False\n        self.h3_is_ready=True\n        \n    def start_a(self,attrs):\n        if self.h3 or self.div:\n            self.a=True\n    def end_a(self):\n        self.a=False\n        \n    def start_div(self,attrs):\n        if self.h3_is_ready == False:\n            return\n        if self.div==True:\n            self.depth += 1\n            \n        for k,v in attrs:\n            if k == 'class' and v == 'content':\n                self.div=True;\n                self.h3_and_div=True   #h3 and div is connected\n    def end_div(self):\n        if self.depth == 0:\n            self.div=False\n            self.h3_and_div=False\n            self.h3_is_ready=False\n            self.names=""\n        if self.div == True:\n            self.depth-=1\n    def handle_data(self,text):\n        #record the name\n        if self.h3 and self.a:\n            self.names+=text\n        #record says\n        if self.h3 and (self.a==False):\n            if not text:pass\n            else: self.dic.setdefault(self.names,[]).append(text)\n            return \n        if self.h3_and_div:\n            self.dic.setdefault(self.names,[]).append(text)\n            \n    def show(self):\n        type = sys.getfilesystemencoding()\n        for key in self.dic:\n            print ( (''.join(key)).replace(' ','')).decode('utf-8').encode(type), \\\n                  ( (''.join(self.dic[key])).replace(' ','')).decode('utf-8').encode(type)\n\n\n\n\nrenrenspider=spider('18621768169','woshidayanzi91729')\nrenrenspider.login()\nrenrenspider.feed(renrenspider.file)\nrenrenspider.show()
-import urllib\n\nhtml_src = urllib.urlopen('http://www.baidu.com').read()
-print("hello")
-import argparse\nimport getpass\nimport itertools\nimport json\nimport logging\nimport os.path\nimport urllib\nimport urllib2\nimport sys\nimport xml.etree.cElementTree as ET\n\nimport base.api\nimport base.atom\nimport base.log\nimport base.tag_helper\nimport base.url_fetcher\nimport base.worker\n\ndef main():\n  base.log.init()\n  base.atom.init()\n\n  parser = argparse.ArgumentParser(\n      description='Comprehensive archive of a Google Reader account')\n\n  # Credentials\n  parser.add_argument('--use_client_login' ,action='store_true',\n                      help='Instead of OAuth, use ClientLogin for '\n                            'authentication. You will be prompted for a '\n                            'username and password')\n  parser.add_argument('--oauth_refresh_token', default='',\n                      help='A previously obtained refresh token (used to bypass '\n                            'OAuth setup')\n  parser.add_argument('--account', default='',\n                      help='Google Account to save the archive for. Omit to '\n                          'specify via standard input')\n  parser.add_argument('--password', default='',\n                      help='Password for the account. Omit to specify via '\n                          'standard input')\n\n  # Output options\n  parser.add_argument('--output_directory', default='./',\n                      help='Directory where to place archive data.')\n\n  # Fetching options\n  parser.add_argument('--stream_items_chunk_size', type=int, default=10000,\n                      help='Number of items refs to request per stream items '\n                           'API call (higher is more efficient)')\n  parser.add_argument('--item_bodies_chunk_size', type=int, default=250,\n                      help='Number of items refs per request for fetching their '\n                           'bodies (higher is more efficient)')\n  parser.add_argument('--comments_chunk_size', type=int, default=250,\n                      help='Number of items per request for fetching comments '\n                           'on shared items (higher is more efficient)')\n  parser.add_argument('--max_streams', type=int, default=0,\n                      help='Maxmium number of streams to archive (0 for no'\n                           'limit, only mean to be used for development)')\n  parser.add_argument('--parallelism', type=int, default=10,\n                      help='Number of requests to make in parallel.')\n\n\n  # Miscellaneous.\n  parser.add_argument('--additional_item_refs_file_path', default='',\n                      help='Path to JSON file listing additional tag item refs '\n                           'to fetch')\n\n  args = parser.parse_args()\n\n  output_directory = base.paths.normalize(args.output_directory)\n  base.paths.ensure_exists(output_directory)\n  def output_sub_directory(name):\n    directory_path = os.path.join(output_directory, name)\n    base.paths.ensure_exists(directory_path)\n    return directory_path\n  api_responses_directory = output_sub_directory('_raw_data')\n  streams_directory = output_sub_directory('streams')\n  data_directory = output_sub_directory('data')\n  items_directory = output_sub_directory('items')\n  comments_directory = output_sub_directory('comments')\n\n  if args.use_client_login:\n    authenticated_url_fetcher = base.url_fetcher.ClientLoginUrlFetcher(\n        args.account, args.password)\n  else:\n    authenticated_url_fetcher = base.url_fetcher.OAuthUrlFetcher(\n        args.oauth_refresh_token)\n  api = base.api.Api(\n      authenticated_url_fetcher=authenticated_url_fetcher,\n      cache_directory=api_responses_directory)\n\n  user_info = api.fetch_user_info()\n  logging.info(\n    'Created API instance for %s (%s)', user_info.user_id, user_info.email)\n\n  logging.info('Saving preferences')\n  _save_preferences(api, data_directory)\n\n  logging.info('Gathering streams to fetch')\n  stream_ids = _get_stream_ids(api, user_info.user_id, data_directory)\n  if args.max_streams and len(stream_ids) > args.max_streams:\n    stream_ids = stream_ids[:args.max_streams]\n  logging.info('%d streams to fetch, gathering item refs:', len(stream_ids))\n\n  fetched_stream_ids = [0]\n  def report_item_refs_progress(stream_id, item_refs):\n    if item_refs is None:\n      logging.error('  Could not load item refs from %s', stream_id)\n      return\n    fetched_stream_ids[0] += 1\n    logging.info('  Loaded %s item refs from %s, %d streams left.',\n        '{:,}'.format(len(item_refs)),\n        stream_id,\n        len(stream_ids) - fetched_stream_ids[0])\n  item_refs_responses = base.worker.do_work(\n      lambda: FetchItemRefsWorker(api, args.stream_items_chunk_size),\n      stream_ids,\n      args.parallelism,\n      report_progress=report_item_refs_progress)\n\n  if args.additional_item_refs_file_path:\n    _load_additional_item_refs(\n        base.paths.normalize(args.additional_item_refs_file_path),\n        stream_ids,\n        item_refs_responses,\n        user_info.user_id)\n\n  item_ids = set()\n  known_item_ids_in_compact_form = set()\n  item_refs_total = 0\n  for stream_id, item_refs in itertools.izip(stream_ids, item_refs_responses):\n    if not item_refs:\n      continue\n    item_ids.update([item_ref.item_id for item_ref in item_refs])\n    item_refs_total += len(item_refs)\n\n    if stream_id == base.api.EXPLORE_STREAM_ID:\n      base.api.not_found_items_ids_to_ignore.update(\n          [i.item_id for i in item_refs])\n\n    stream = base.api.Stream(stream_id=stream_id, item_refs=item_refs)\n    stream_file_name = base.paths.stream_id_to_file_name(stream_id) + '.json'\n    stream_file_path = os.path.join(streams_directory, stream_file_name)\n    with open(stream_file_path, 'w') as stream_file:\n      stream_file.write(json.dumps(stream.to_json()))\n\n  item_ids = list(item_ids)\n  logging.info('%s unique items refs (%s total), getting item bodies:',\n      '{:,}'.format(len(item_ids)),\n      '{:,}'.format(item_refs_total))\n\n  # We have two different chunking goals:\n  # - Fetch items in large-ish chunks (ideally 250), to minimize HTTP request\n  #   overhead per item\n  # - Write items in small-ish chunks (ideally around 10) per file, since having\n  #   a file per item is too annoying to deal with from a file-system\n  #   perspective. We also need the chunking into files to be deterministic, so\n  #   that from an item ID we know what file to look for it in.\n  # We therefore first chunk the IDs by file path, and then group those chunks\n  # into ID chunks that we fetch.\n  # We write the file chunks immediately after fetching to decrease the\n  # in-memory working set of the script.\n  item_ids_by_path = {}\n  for item_id in item_ids:\n    item_id_file_path = base.paths.item_id_to_file_path(\n        items_directory, item_id)\n    item_ids_by_path.setdefault(item_id_file_path, []).append(item_id)\n\n  current_item_ids_chunk = []\n  item_ids_chunks = [current_item_ids_chunk]\n  for item_ids_for_file_path in item_ids_by_path.values():\n    if len(current_item_ids_chunk) + len(item_ids_for_file_path) > \\\n          args.item_bodies_chunk_size:\n      current_item_ids_chunk = []\n      item_ids_chunks.append(current_item_ids_chunk)\n    current_item_ids_chunk.extend(item_ids_for_file_path)\n\n  item_bodies_to_fetch = len(item_ids)\n  fetched_item_bodies = [0]\n  def report_item_bodies_progress(_, count):\n    if count is None:\n      return\n    fetched_item_bodies[0] += count\n    logging.info('  Fetched %s/%s item bodies',\n        '{:,}'.format(fetched_item_bodies[0]),\n        '{:,}'.format(item_bodies_to_fetch))\n  base.worker.do_work(\n      lambda: FetchWriteItemBodiesWorker(api, items_directory),\n      item_ids_chunks,\n      args.parallelism,\n      report_progress=report_item_bodies_progress)\n\n  broadcast_stream_ids = [\n      stream_id for stream_id in stream_ids\n      if stream_id.startswith('user/') and\n          stream_id.endswith('/state/com.google/broadcast')\n  ]\n  logging.info(\n      'Fetching comments from %d shared item streams.',\n      len(broadcast_stream_ids))\n  encoded_sharers = api.fetch_encoded_sharers()\n  remaining_broadcast_stream_ids = [len(broadcast_stream_ids)]\n  def report_comments_progress(_, comments_by_item_id):\n    if comments_by_item_id is None:\n      return\n    remaining_broadcast_stream_ids[0] -= 1\n    comment_count = sum((len(c) for c in comments_by_item_id.values()), 0)\n    logging.info('  Fetched %s comments, %s shared items streams left.',\n        '{:,}'.format(comment_count),\n        '{:,}'.format(remaining_broadcast_stream_ids[0]))\n  all_comments = {}\n  comments_for_broadcast_streams = base.worker.do_work(\n      lambda: FetchCommentsWorker(\n          api, encoded_sharers, args.comments_chunk_size),\n      broadcast_stream_ids,\n      args.parallelism,\n      report_progress=report_comments_progress)\n  total_comment_count = 0\n  for comments_for_broadcast_stream in comments_for_broadcast_streams:\n    if not comments_for_broadcast_stream:\n      continue\n    for item_id, comments in comments_for_broadcast_stream.iteritems():\n      total_comment_count += len(comments)\n      all_comments.setdefault(item_id, []).extend(comments)\n\n  logging.info('Writing %s comments from %s items.',\n      '{:,}'.format(total_comment_count),\n      '{:,}'.format(len(all_comments)))\n  for item_id, comments in all_comments.items():\n    item_comments_file_path = os.path.join(base.paths.item_id_to_file_path(\n        comments_directory, item_id), item_id.compact_form())\n    base.paths.ensure_exists(os.path.dirname(item_comments_file_path))\n    with open(item_comments_file_path, 'w') as item_comments_file:\n      item_comments_file.write(json.dumps([c.to_json() for c in comments]))\n\n  with open(os.path.join(output_directory, 'README'), 'w') as readme_file:\n    readme_file.write('See https://github.com/mihaip/readerisdead/'\n        'wiki/reader_archive-Format.\\n')\n\ndef _save_preferences(api, data_directory):\n  def save(preferences_json, file_name):\n    file_path = os.path.join(data_directory, file_name)\n    with open(file_path, 'w') as file:\n      file.write(json.dumps(preferences_json))\n\n  save(api.fetch_preferences(), 'preferences.json')\n  save(api.fetch_stream_preferences(), 'stream-preferences.json')\n  save(\n      [g.to_json() for g in api.fetch_sharing_groups()], 'sharing-groups.json')\n  save(api.fetch_sharing_acl().to_json(), 'sharing-acl.json')\n\ndef _get_stream_ids(api, user_id, data_directory):\n  def save_items(items, file_name):\n    file_path = os.path.join(data_directory, file_name)\n    with open(file_path, 'w') as file:\n      file.write(json.dumps([i.to_json() for i in items]))\n\n  stream_ids = set()\n\n  tags = api.fetch_tags()\n  tag_stream_ids = set([t.stream_id for t in tags])\n  for system_tag in base.tag_helper.TagHelper(user_id).system_tags():\n    if system_tag.stream_id not in tag_stream_ids:\n      tags.append(system_tag)\n      tag_stream_ids.add(system_tag.stream_id)\n  stream_ids.update([tag.stream_id for tag in tags])\n  save_items(tags, 'tags.json')\n\n  subscriptions = api.fetch_subscriptions()\n  stream_ids.update([sub.stream_id for sub in subscriptions])\n  save_items(subscriptions, 'subscriptions.json')\n\n  friends = api.fetch_friends()\n  stream_ids.update([\n      f.stream_id for f in friends if f.stream_id and f.is_following])\n  save_items(friends, 'friends.json')\n\n  bundles = api.fetch_bundles()\n  for bundle in bundles:\n    stream_ids.update([f.stream_id for f in bundle.feeds])\n  save_items(bundles, 'bundles.json')\n\n  recommendations = api.fetch_recommendations()\n  stream_ids.update([r.stream_id for r in recommendations])\n  save_items(recommendations, 'recommendations.json')\n\n  stream_ids.add(base.api.EXPLORE_STREAM_ID)\n\n  stream_ids = list(stream_ids)\n  # Start the fetch with user streams, since those tend to have more items and\n  # are thus the long pole.\n  stream_ids.sort(reverse=True)\n  return stream_ids\n\ndef _load_additional_item_refs(\n    additional_item_refs_file_path, stream_ids, item_refs_responses, user_id):\n  logging.info('Adding additional item refs.')\n  compact_item_ids_by_stream_id = {}\n  item_refs_responses_by_stream_id = {}\n  for stream_id, item_refs in itertools.izip(stream_ids, item_refs_responses):\n    compact_item_ids_by_stream_id[stream_id] = set(\n      item_ref.item_id.compact_form() for item_ref in item_refs)\n    item_refs_responses_by_stream_id[stream_id] = item_refs\n\n  # The JSON file stores item IDs in hex, but with a leading 0x. Additionally,\n  # timestamps are in microseconds, but they're stored as strings.\n  def item_ref_from_json(item_ref_json):\n      return base.api.ItemRef(\n        item_id=base.api.item_id_from_compact_form(item_ref_json['id'][2:]),\n        timestamp_usec=int(item_ref_json['timestampUsec']))\n\n  with open(additional_item_refs_file_path) as additional_item_refs_file:\n    additional_item_refs = json.load(additional_item_refs_file)\n    for stream_id, item_refs_json in additional_item_refs.iteritems():\n      if not stream_id.startswith('user/%s/' % user_id) or \\\n          'state/com.google/touch' in stream_id or \\\n          'state/com.google/recommendations-' in stream_id:\n        # Ignore tags from other users and those added by\n        # https://github.com/mihaip/google-reader-touch. Also ignore the\n        # recommendations tags, the items that they refer to aren't actually\n        # items in the Reader backend.\n        continue\n\n      if stream_id not in item_refs_responses_by_stream_id:\n        logging.info('  Stream %s (%s items) is new.',\n          stream_id, '{:,}'.format(len(item_refs_json)))\n        stream_ids.append(stream_id)\n        item_refs_responses.append(\n            [item_ref_from_json(i) for i in item_refs_json])\n      else:\n        new_item_refs = []\n        alread_known_item_ref_count = 0\n        known_item_ids = compact_item_ids_by_stream_id[stream_id]\n        for item_ref_json in item_refs_json:\n          if item_ref_json['id'] == '0x859df8b8d14b566e':\n            # Skip this item, it seems to cause persistent 500s\n            continue\n          if item_ref_json['id'][2:] not in known_item_ids:\n            new_item_refs.append(item_ref_from_json(item_ref_json))\n          else:\n            alread_known_item_ref_count += 1\n        if new_item_refs:\n          logging.info('  Got an additional %s item refs for %s '\n              '(%s were already known)',\n              '{:,}'.format(len(new_item_refs)),\n              stream_id,\n              '{:,}'.format(alread_known_item_ref_count))\n          item_refs_responses_by_stream_id[stream_id].extend(new_item_refs)\n\nclass FetchItemRefsWorker(base.worker.Worker):\n  def __init__(self, api, chunk_size):\n    self._api = api\n    self._chunk_size = chunk_size\n\n  def work(self, stream_id):\n    result = []\n    continuation_token = None\n    while True:\n      item_refs, continuation_token = self._api.fetch_item_refs(\n          stream_id,\n          count=self._chunk_size,\n          continuation_token=continuation_token)\n      result.extend(item_refs)\n      if not continuation_token:\n        break\n    return result\n\nclass FetchWriteItemBodiesWorker(base.worker.Worker):\n  def __init__(self, api, items_directory):\n    self._api = api\n    self._items_directory = items_directory\n\n  def work(self, item_ids):\n    if not item_ids:\n      return 0\n\n    item_bodies_by_id = self._fetch_item_bodies(item_ids)\n    if not item_bodies_by_id:\n      return 0\n\n    item_bodies_by_file_path = self._group_item_bodies(\n      item_bodies_by_id.values())\n    for file_path, item_bodies in item_bodies_by_file_path.items():\n      self._write_item_bodies(file_path, item_bodies)\n    return len(item_bodies_by_id)\n\n  def _fetch_item_bodies(self, item_ids):\n    def fetch(hifi=True):\n      result = self._api.fetch_item_bodies(\n              item_ids,\n              format='atom-hifi' if hifi else 'atom',\n              # Turn off authentication in order to make the request cheaper/\n              # faster. Item bodies are not ACLed, we already have per-user tags\n              # via the stream item ref fetches, and will be fetching comments\n              # for shared items separately.\n              authenticated=False)\n      return result\n\n    try:\n      try:\n        return fetch()\n      except urllib2.HTTPError, e:\n        if e.code == 500:\n          logging.info('  500 response when fetching items, retrying with '\n              'high-fidelity output turned off')\n          return fetch(hifi=False)\n        else:\n          logging.error('  HTTP exception when fetching items', exc_info=True)\n          return None\n      except ET.ParseError, e:\n          logging.info('  XML parse error when fetching items, retrying with '\n              'high-fidelity turned off')\n          return fetch(hifi=False)\n    except:\n      logging.error('  Exception when fetching items', exc_info=True)\n      return None\n\n  def _group_item_bodies(self, item_bodies):\n    item_bodies_by_path = {}\n    for entry in item_bodies:\n      item_id_file_path = base.paths.item_id_to_file_path(\n          self._items_directory, entry.item_id)\n      item_bodies_by_path.setdefault(item_id_file_path, []).append(entry)\n    return item_bodies_by_path\n\n  def _write_item_bodies(self, file_path, item_bodies):\n    base.paths.ensure_exists(os.path.dirname(file_path))\n    feed_element = ET.Element('{%s}feed' % base.atom.ATOM_NS)\n    for entry in item_bodies:\n      feed_element.append(entry.element)\n\n    with open(file_path, 'w') as items_file:\n        ET.ElementTree(feed_element).write(\n            items_file,\n            xml_declaration=True,\n            encoding='utf-8')\n\nclass FetchCommentsWorker(base.worker.Worker):\n  def __init__(self, api, encoded_sharers, chunk_size):\n    self._api = api\n    self._encoded_sharers = encoded_sharers\n    self._chunk_size = chunk_size\n\n  def work(self, broadcast_stream_id):\n    result = {}\n    continuation_token = None\n    while True:\n      comments_by_item_id, continuation_token = self._api.fetch_comments(\n          broadcast_stream_id,\n          encoded_sharers=self._encoded_sharers,\n          count=self._chunk_size,\n          continuation_token=continuation_token)\n      result.update(comments_by_item_id)\n      if not continuation_token:\n        break\n    return result\n\nif __name__ == '__main__':\n    main()
-stevenlordiam@gmail.com
-LWYleoapple750703
-stevenlordiam@gmail.com
-LWYleoapple750703
